[
  {
    "objectID": "data-products.html",
    "href": "data-products.html",
    "title": "Data products",
    "section": "",
    "text": "Describe GeoServer, GeoNode and databases (PostGIS)\nHow do we move data uploaded to MinIO to GeoServer?\nHow do we expose orthomosaics and vector datasets as web services (WMS and WFS)? How do we add them to e.g. ArcGIS?\nWhat is the difference between registered and unregistered users on the GeoNode site?\nWhat can users do via GeoNode and how do they do it (create new maps, download data… ?)"
  },
  {
    "objectID": "jupyterhub.html",
    "href": "jupyterhub.html",
    "title": "JupyterHub",
    "section": "",
    "text": "How much “personal” storage do users have (and where is it stored)?\nWhat buckets on MinIO can they access?\nAre their files backed up?\nWhat level of computing resources can they access (CPU, memory, GPU)?\nWhat packages are available in the user environment (for both Python and R)?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SeaBee documentation",
    "section": "",
    "text": "This website provides technical documentation for various aspects of the SeaBee workflow.\nTo learn more about the SeaBee project itself, please visit https://seabee.no/."
  },
  {
    "objectID": "index.html#sec-overview",
    "href": "index.html#sec-overview",
    "title": "SeaBee documentation",
    "section": "1 Overview",
    "text": "1 Overview\nThe simplest possible version of the SeaBee data flow is illustrated in Figure 1. Geotagged images from aerial drones are combined with ground control points using orthorectification software to produce georeferenced image mosaics.\nGround-truth data collected in the field are used to aid annotation of the mosaics to create training datasets for machine learning algorithms. These algorithms are applied to generate data products, which are made available via the SeaBee GeoNode.\n\n\n\nFigure 1: Simplified SeaBee workflow.\n\n\nThe full SeaBee workflow is considerably more complex than illustrated above. In particular, note the following:\n\nStandard aerial drones are flown with both RGB and multispectral cameras. In addition, the project includes hyperspectral data as well as imagery from other types of drone, such as the Otter (an unmanned surface vehicle).\nTypical data products from the machine learning include both image segmentation (e.g. habitat mapping) and object identification (e.g. seabird or mammal counts).\n\nAs far as possible, data handling for the SeaBee project is hosted by Sigma2 on NIRD, the National Infrastructure for Research Data."
  },
  {
    "objectID": "storage.html",
    "href": "storage.html",
    "title": "Storage",
    "section": "",
    "text": "Describe MinIO instance, how to login, basic bucket/folder structure etc.\nS3-compatible storage endpoint at https://storage.seabee.sigma2.no\nWeb interface at https://minio.seabee.sigma2.no/login"
  },
  {
    "objectID": "storage.html#backups",
    "href": "storage.html#backups",
    "title": "Storage",
    "section": "2 Backups",
    "text": "2 Backups\n\nDescribe backup schedule"
  },
  {
    "objectID": "storage.html#uploading-and-downloading-files",
    "href": "storage.html#uploading-and-downloading-files",
    "title": "Storage",
    "section": "3 Uploading and downloading files",
    "text": "3 Uploading and downloading files\n\nDescribe options for uploading and download, including detailed configuration steps\n\n\n3.1 MinIO web interface\n\nOverview of the web UI\nHow to manually browse, upload and download files\nWhat should go where?\nHow should files be structured?\n\n\n\n3.2 SeaBee data upload portal\n\nDescribe Deb’s data upload portal\nBest option for standard/less advanced users?\n\n\n\n3.3 Python API\n\nPossible to up/download using Python?\nCan this be done from an external machine, or only from the SeaBee JupyterHub?\nBased on Kim’s example here\n\n\n\n3.4 Rclone\nRclone provides a convenient way of synchronising local files to cloud storage. It is probably the best option for experienced/advanced users to add large volumes of imagery to Sigma2.\n\nDescribe the configuration required to get Rclone working with MinIO on Sigma2"
  },
  {
    "objectID": "howto.html",
    "href": "howto.html",
    "title": "How to",
    "section": "",
    "text": "I want to…"
  },
  {
    "objectID": "howto.html#sec-create-account",
    "href": "howto.html#sec-create-account",
    "title": "How to",
    "section": "1 Create a user account",
    "text": "1 Create a user account\nYou need to create a Feide account and then request access to the resources you will need. See the Login page for details."
  },
  {
    "objectID": "howto.html#sec-file-transfer",
    "href": "howto.html#sec-file-transfer",
    "title": "How to",
    "section": "2 Upload or download files",
    "text": "2 Upload or download files\nThe best option will depend on your level of technical expertise and the volume of data you wish to transfer. Non-technical users with smaller data volumes should consider either the MinIO web interface or the SeaBee data upload portal. For larger volumes of data (e.g. raw imagery for whole missions), command-line solutions such as Rclone are preferable. See Uploading and downloading files for details."
  },
  {
    "objectID": "howto.html#sec-orthorectification",
    "href": "howto.html#sec-orthorectification",
    "title": "How to",
    "section": "3 Mosaic geotagged raw images to georeferenced mosaics",
    "text": "3 Mosaic geotagged raw images to georeferenced mosaics\nYou can use Open Drone Map (ODM) via either PyODM or CloudODM (e.g. via the SeaBee JupyterHub). See the Orthorectification page for details."
  },
  {
    "objectID": "howto.html#sec-annotation",
    "href": "howto.html#sec-annotation",
    "title": "How to",
    "section": "4 Create training data for machine learning (annotation)",
    "text": "4 Create training data for machine learning (annotation)\nThe recommended workflow for image segmentation tasks is to use ArcGIS Pro and the Image Analyst extension - see the Annotation page for details."
  },
  {
    "objectID": "howto.html#explore-and-download-seabee-datasets",
    "href": "howto.html#explore-and-download-seabee-datasets",
    "title": "How to",
    "section": "5 Explore and download SeaBee datasets",
    "text": "5 Explore and download SeaBee datasets\nOrthomosaics from aerial drones and data products from the machine learning are available via SeaBee’s GeoNode. See the Data products page for details."
  },
  {
    "objectID": "orthorectification.html",
    "href": "orthorectification.html",
    "title": "Orthorectification",
    "section": "",
    "text": "Orthorectification within SeaBee is currently performed using two main tools: Open Drone Map (ODM), which is Open Source, and Pix4D, which is proprietary. To date, orthorectification for habitat mapping (image segmentation) has been handled by drone pilots using Pix4D, whereas orthorectification for seabird counts (object identification) has been done by researchers using ODM.\nPix4D cannot be deployed on Sigma2 due to licensing restrictions, but ODM is available via the NodeODM API. PyODM and CloudODM are also provided."
  },
  {
    "objectID": "orthorectification.html#open-drone-map",
    "href": "orthorectification.html#open-drone-map",
    "title": "Orthorectification",
    "section": "1 Open Drone Map",
    "text": "1 Open Drone Map\n\nDescribe the configuration for ODM on Sigma2\nAdd simple examples of mosaicing test images from a MinIO bucket using both Python (PyODM) and the command line (CloudODM)\nBest done via the JupyterHub interface?"
  },
  {
    "objectID": "login.html",
    "href": "login.html",
    "title": "Login",
    "section": "",
    "text": "Access to SeaBee’s data platform is controlled via Feide. You can login via Dataporten here.\nSome organisations automatically provide Feide access for all their memebrs. If you do not have organisational access, you will need to create a Fedie Guest account (see Section 1)."
  },
  {
    "objectID": "login.html#sec-create-account",
    "href": "login.html#sec-create-account",
    "title": "Login",
    "section": "1 Create a Feide account",
    "text": "1 Create a Feide account\n\nRegister a new account with Feide here\n\n\nDescribe this part in more detail"
  },
  {
    "objectID": "login.html#sec-minio",
    "href": "login.html#sec-minio",
    "title": "Login",
    "section": "2 Browse files using MinIO",
    "text": "2 Browse files using MinIO\nSeaBee uses MinIO on Sigma2 to provide an S3-compatible API for file storage and transfer. Once you have a Feide account, you can request access to MinIO by creating an issue in the SeaBee repository explaining your use case and describing which datasets you need to access.\nThe easiest way to explore the files available to you on MinIO is via the web interface here. See the File storage page for further details, including alternative options for interacting with SeaBee data on Sigma2."
  },
  {
    "objectID": "login.html#sec-jupyterhub",
    "href": "login.html#sec-jupyterhub",
    "title": "Login",
    "section": "3 Data processing using SeaBee’s JupterHub",
    "text": "3 Data processing using SeaBee’s JupterHub\nSeaBee’s JupyterHub provides Python and R environments offering fast and convenient access to data hosted on Sigma2. The JupyterHub is especially useful for developing and testing workflows and for sharing code with other researchers. Once you have a Feide account, you can request access to JupyterHub by creating an issue in the SeaBee repository explaining your use case. You can then login here using your Feide credentials.\nSee the JupyterHub page for full details."
  },
  {
    "objectID": "login.html#sec-geonode",
    "href": "login.html#sec-geonode",
    "title": "Login",
    "section": "4 Editor permissions on GeoNode",
    "text": "4 Editor permissions on GeoNode\nSeaBee data products are publicly available via the SeaBee Geonode.\n\nDescribe what happens when users click the Register button on the GeoNode site i.e. what can registered users do than unregisterd users cannot?"
  },
  {
    "objectID": "annotation.html",
    "href": "annotation.html",
    "title": "Annotation",
    "section": "",
    "text": "Repository on GitHub"
  },
  {
    "objectID": "annotation.html#sec-overview",
    "href": "annotation.html#sec-overview",
    "title": "Annotation",
    "section": "1 Overview",
    "text": "1 Overview\nThis document describes the SeaBee annotation workflow developed and tested using the Runde/Remøy dataset during December 2022.\nThe current workflow is preliminary and should become smoother once integrated with Sigma2. The aim of these notes is to help new users get started with annotation using ArcGIS Pro and to avoid common mistakes/pitfalls."
  },
  {
    "objectID": "annotation.html#sec-workflow",
    "href": "annotation.html#sec-workflow",
    "title": "Annotation",
    "section": "2 Workflow",
    "text": "2 Workflow\n\n2.1 Install ArcGIS Pro\nFollow the steps below to download and install ArcGIS Pro on your local machine.\n\nContact Jan Karud to obtain login details for ArcGIS Online. You need access to ArcGIS Pro with the Image Analyst extension.\nLogin to ArcGIS Online and, under your profile, choose Settings > Licenses. Check that Image Analyst is available to you under ArcGIS Pro extensions, then click the link to Download ArcGIS Pro (Figure 1).\nObtain an administrator password from IT-Vakt and run the installer. You should then be able to start ArcGIS Pro and login to the application using your ArcGIS Online credentials.\n\n\n\n\nFigure 1: Download ArcGIS Pro\n\n\n\n\n2.2 Setup project\nThe next step is to create a new project within ArcGIS and add SeaBee data to it.\n\nIn ArcGIS Pro, create a new Map project. The Name should describe the mission/area you’re annotating and Location should be an existing local folder on your PC.\n\n\n\n\n\n\n\nTip\n\n\n\nArcGIS may run slowly if you set Location to be a network folder or a folder that is synchronised to an external server (e.g. DropBox, OneDrive, GoogleDrive).\n\n\n\nWithin your project folder, create three new subfolders: class_definitions, vector and raster. This can either be done using the Catalog pane in ArcGIS or using Windows’ File Explorer.\nDownload relevant mission datasets and add them to the appropriate subfolders. As a minimum, you will need a class definition file and a georeferenced orthomosaic. Optionally, you may include a ground truth dataset, a region of interest (ROI) file and any pre-existing annotation for your area of interest. See Section 2.3 to Section 2.6 for details.\n\n\n\n\n\n\n\nNote\n\n\n\nAt present, most mission data is hosted on OneDrive/Sharepoint, which means it must be downloaded locally for use with ArcGIS. For large files, this can be slow. Eventually, datasets should be hosted on Sigma2 and exposed as web mapping services (WMS). This will make it possible to add the orthomosaics to your map without having to download large volumes of raw imagery.\n\n\n\n\n2.3 Class definition files\nArcGIS Pro supports hierarchical class definitions, which can be defined manually via the Training Samples Manager. Class definitions are saved as ESRI Classification Schema files (.ecs), which are JSON files with a specific structure.\nThe classes of interest to SeaBee are complex and creating them manually via the user interface is cumbersome. Section 2 of the notebook here includes code to build an .ecs file with the correct schema from an Excel table, which is more convenient in most cases.\nAs far as possible, SeaBee will use a standard set of class definitions for habitat mapping. The latest versions are available online here in both Excel and .ecs formats.\nThe workflow for class definition files is as follows:\n\nBefore starting to annotate a new area, everyone involved must agree on which set of class definitions (i.e. which version) to use. If necessary, class definitions can be updated in Excel and a new version of the .ecs file created using the code linked above.\nEveryone should download the same .ecs file and add it to the class_definitions folder in their ArcGIS project (created in Section 2.2).\n\n\n\n\n\n\n\nImportant\n\n\n\nUsing a standard set of classes is important if the machine learning algorithms created by SeaBee are to be transferable/re-trainable. It is likely that some changes to the class schema will be necessary initially, but we are hoping to converge on a standard set of habitat classes if possible. Proposed changes should be discussed with Hege Gundersen and Kristina Øie Kvile.\n\n\n\n\n2.4 Orthomosaics\nOrthomosaics for your area of interest should be downloaded and added to the raster folder within your ArcGIS project. In most cases, the RGB mosaics are most useful for annotation, since they look familiar and have the highest resolution. Multispectral data may be worth including in some cases, although it has not been used much for annotation so far.\nData are currently available via Sharepoint/Teams in a folder structure that typically looks something like this:\nWP4 > 1_DATA-SeaBee > {year} > {mission} > 1_drone > {pilot/organisation} > {time_spec_altitude} > Mosaics\nA direct link to the 1_DATA-SeaBee folder is here.\n\n\n\n\n\n\nTip\n\n\n\nDownloading large files from Teams/Sharepoint via the UI can be slow and unstable. Experience with the Runde data suggests the Python API is faster and more reliable. The notebook here shows an example of this.\n\n\n\n\n\n\n\n\nNote\n\n\n\nEventually, it is hoped that all SeaBee orthomosiacs will be available as WMS layers from Geoserver running on Sigma2. This will make it possible to add raster imagery to ArcGIS without downloading it locally.\n\n\n\n\n2.5 Ground truth data\nPoint shapefiles of ground truth data are available for most missions. These are typically stored in the mission folder on Sharepoint/Teams within a subfolder named GT.\nIf available, download the shapefile and add it to the vector folder within your in ArcGIS project.\n\n\n2.6 Region of interest and training subareas\nFor each mission, you should create shapefiles defining (i) your region of interest (ROI), and (ii) a set of subareas that will be used to divide the annotation data into “blocks”. When you create these shapefiles, be sure to use the same co-ordinate reference system (CRS) as the orthomosaic you wish to annotate.\n\nThe ROI defines the area that you would eventually like to classify. This will typically cover a large proportion of the total image, but excluding anything not covered by your class definition file (see Section 2.3). As an example, see the black dashed line defining the ROI for Remøy on Figure 2.\nMachine learning algorithms only learn based on the training samples you provide, so if the prediction area includes things not present in the training data you will get poor results. If possible, use the ROI file to define an area excluding things like roads, buildings and bridges that you are not interested in. You can then ignore these at the annotation stage and focus instead on annotating classes of ecological interest.\n\n\n\n\n\n\n\nImportant\n\n\n\nIf your area includes lots of man-made objects and you can’t exclude them using an ROI for some reason, make sure you annotate a representative selection of each type of object and tag them all as ANTHRO.\n\n\n\nThe training subareas are a set of rectangles (say, 5 to 10) that define discrete areas within which you create annotation. See the red rectangles on Figure 2 for an example from Remøy. Defining subareas is helpful because we might want to train an algorithm using annotations from e.g. Areas 1 to 3, then iteratively evaluate it against data from Areas 4 and 5. Once we’re satisfied, we can use data from Area 6 to get a independent assessment of the model’s performance.\nDefining training subareas is also a convenient way of dividing the annotation workload between several people: each person agrees to annotate one or two specific subareas. This avoids accidental duplication of effort (although at times it may also be useful for several people to annotate the same area to assess consistency).\n\n\n\n\nFigure 2: Region of interest and training subareas used for Remøy\n\n\n\n\n2.7 Styling the map\nOnce you have download all the relevant datasets and added them to your project folder in ArcGIS, you can add layers to your map and style them appropriately. Figure 3 shows an example.\n\nDrag each layer from the Catalog pane (right-most column in Figure 3) to the ArcGIS Table of contents (left-most pane in Figure 3). The order of layers in the Table of contents defines the drawing order on the map, so put the orthomosaic at the bottom and the other layers on top.\nRight-click each vector layer in the Table of contents and choose Symbology. This will allow you to define fill and outline colours for the vector layers on your map.\n[Optional] Right-click the ground truth dataset and choose Labelling Properties. Set the Expression to be $feature.Kode, where Kode is the name of the column in the ground truth attribute table containing the labels you want to use. When you click Apply, you should see each point in the ground truth dataset labelled with its class code.\n\n\n\n\n\n\n\nTip\n\n\n\nWhenever you’re working with anything GIS-related, remember to save your work regularly!\n\n\n\n\n\nFigure 3: Example ArcGIS layout\n\n\n\n\n2.8 Creating annotation\nWith the ArcGIS project configured, you can begin creating annotation.\n\nIn the ArcGIS Table of contents (left-most pane in Figure 3), select one of your orthomosaic layers. This will activate the Image Analyst extension.\nOn the “ribbon” (i.e. menu bar), select the Imagery tab and choose Classification Tools > Training Samples Manager. A new pane should appear at the right side of the window.\nIn the upper part of the new pane, click the folder icon (which has a tooltip saying Classification schema) and load your class definition file (see Section 2.3). You should see the class hierarchy added to the upper half of the window.\nIdentify the training area you wish to annotate and zoom in on a feature (e.g. a boulder or patch of algae). In the class hierarchy, select the class you wish to annotate, choose one of drawing tools from the top of the window and begin digitising. In most cases, the Freehand tool is likely to be most useful.\n\nEach polygon you draw will appear in the lower half of the Image Classification pane (see Figure 4). The Pixels (%) column shows what proportion of the pixels digitised so far belong to each class.\n\n\n\nFigure 4: The Training Samples Manager\n\n\n\nIt is a good idea to periodically group polygons of the same class. This is done by selecting the rows you wish to group in the lower pane (using SHIFT + Click or CTRL + Click) and then clicking the Collapse icon (two arrows coming together). You can also ungroup using the Expand button (one arrow splitting into two).\nWhen you have finished your digitising session, click the Save icon in the lower pane of the Training Samples Manager to save your training samples as a shapefile in the vector folder of your ArcGIS project. You should also save the entire ArcGIS project before closing down.\nIf you wish to continue annotating using a shapefile created previously, first open your ArcGIS project and load the class definitions file (steps 1 to 3 above). Then, instead of creating new annotation from scratch, click the folder icon in the lower part of the Image Classification pane (labelled Load training samples) and browse to the annotation shapefile created previously. You can now continue annotating and save changes back to the original shapefile.\n\n\n\n\n\n\n\nTip\n\n\n\nThe following tips should help you to create good quality annotation:\n\nAlways assign the most detailed level in the class hierarchy that you can confidently identify. If you are not sure, assign the level above.\nGroup your polygons by class regularly and get into the habit of clicking Save immediately before each grouping operation.\nUse the Pixels (%) column to prioritise which classes to focus on. Given the classes of interest, you will probably not be able to produce a “balanced” training dataset, but if you have e.g. 90% BOULDER there’s no point digitising more boulders.\nYou don’t need to digitise everything within each training subarea (but the more the better).\nDo not annotate anything outside of the training subareas and do not draw polygons that cross subarea boundaries.\nDo not draw overlapping polygons or polygons that touch one another (ideally, there should be at least one pixel between adjacent polygons).\nDo not draw self-intersecting polygons (i.e. when the line you’re drawing crosses itself, such as when drawing a figure-of-eight or bow-tie shape). Such polygons are invalid and they cause problems later in the workflow. In particular, there is a bug/lack of error handling in ArcGIS Pro’s Training Samples Manager that causes the application to crash hard if you attempt to group invalid polygons.\nIf you’re not sure how to assign something, or where a boundary should be drawn, the key question to ask yourself is: “Would I be happy if an algorithm classified this entire polygon as X?” If the answer is “Yes”, it is reasonable to tag the whole polygon as X; if the answer is “No” consider subdividing or deleting it.\n\n\n\n\n\n2.9 Packaging annotation\nOnce all subareas have been digitised, each person should upload their annotation shapefiles to an agreed folder on Teams (eventually, on Sigma2). Make sure each subarea is included only once.\n\nWork through the notebook here, specifically Sections 3 to 6. This will:\n\nMerge the annotation shapefiles for each subarea or group of subareas into a single dataset.\nTag each of the annotation polygons with the subarea ID, making it easier to filter/subdivide the training data.\nReconstruct the original, three-column class hierarchy from the single-column ArcGIS output. This makes it easy to generate raster annotation for any of the three “levels”.\nCreate a geopackage combining the annotation data, the subarea polygons and the region of interest.\n\nThe geopackage should be uploaded to Teams/Sharepoint and shared with NR, together with links to the relevant orthomosaics."
  }
]